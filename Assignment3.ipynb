{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d65c9e59",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "1) Train and validate rnn.RNN, rnn.LSTM and rnn.GRU for learning the above sequence. Use sequences of 10, 20, and 30 for your training. Feel free to adjust other network parameters. Report and compare training loss, validation accuracy, execution time for training, and computational and mode size complexities across the three models over various lengths of sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "061fb94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN model with sequence length 10...\n",
      "Model RNN with sequence length 10 - Training Loss: 0.9861, Training Accuracy: 0.7156, Execution Time: 27.37 seconds, Model Size: 28205 parameters\n",
      "\n",
      "Next predicted character for sequence 'Next chara': c\n",
      "\n",
      "Training LSTM model with sequence length 10...\n",
      "Model LSTM with sequence length 10 - Training Loss: 1.3095, Training Accuracy: 0.6109, Execution Time: 36.08 seconds, Model Size: 95405 parameters\n",
      "\n",
      "Next predicted character for sequence 'Next chara': c\n",
      "\n",
      "Training GRU model with sequence length 10...\n",
      "Model GRU with sequence length 10 - Training Loss: 1.0053, Training Accuracy: 0.7308, Execution Time: 57.57 seconds, Model Size: 73005 parameters\n",
      "\n",
      "Next predicted character for sequence 'Next chara': c\n",
      "\n",
      "Training RNN model with sequence length 20...\n",
      "Model RNN with sequence length 20 - Training Loss: 0.9853, Training Accuracy: 0.7305, Execution Time: 51.87 seconds, Model Size: 28205 parameters\n",
      "\n",
      "Next predicted character for sequence 'Next character predi': c\n",
      "\n",
      "Training LSTM model with sequence length 20...\n",
      "Model LSTM with sequence length 20 - Training Loss: 1.5753, Training Accuracy: 0.5632, Execution Time: 58.62 seconds, Model Size: 95405 parameters\n",
      "\n",
      "Next predicted character for sequence 'Next character predi': c\n",
      "\n",
      "Training GRU model with sequence length 20...\n",
      "Model GRU with sequence length 20 - Training Loss: 0.9926, Training Accuracy: 0.6768, Execution Time: 104.92 seconds, Model Size: 73005 parameters\n",
      "\n",
      "Next predicted character for sequence 'Next character predi': c\n",
      "\n",
      "Training RNN model with sequence length 30...\n",
      "Model RNN with sequence length 30 - Training Loss: 1.0965, Training Accuracy: 0.6920, Execution Time: 81.97 seconds, Model Size: 28205 parameters\n",
      "\n",
      "Next predicted character for sequence 'Next character prediction is a': r\n",
      "\n",
      "Training LSTM model with sequence length 30...\n",
      "Model LSTM with sequence length 30 - Training Loss: 1.5959, Training Accuracy: 0.5668, Execution Time: 81.10 seconds, Model Size: 95405 parameters\n",
      "\n",
      "Next predicted character for sequence 'Next character prediction is a': n\n",
      "\n",
      "Training GRU model with sequence length 30...\n",
      "Model GRU with sequence length 30 - Training Loss: 1.0359, Training Accuracy: 0.7166, Execution Time: 159.48 seconds, Model Size: 73005 parameters\n",
      "\n",
      "Next predicted character for sequence 'Next character prediction is a': n\n",
      "\n",
      "                                                               RNN  \\\n",
      "Sequence Length                                                      \n",
      "10               {'loss': 0.9861283521718421, 'accuracy': 0.715...   \n",
      "20               {'loss': 0.9853182879489436, 'accuracy': 0.730...   \n",
      "30               {'loss': 1.09653293906909, 'accuracy': 0.69198...   \n",
      "\n",
      "                                                              LSTM  \\\n",
      "Sequence Length                                                      \n",
      "10               {'loss': 1.3095309579878933, 'accuracy': 0.610...   \n",
      "20               {'loss': 1.5752627657695717, 'accuracy': 0.563...   \n",
      "30               {'loss': 1.5959076870787583, 'accuracy': 0.566...   \n",
      "\n",
      "                                                               GRU  \n",
      "Sequence Length                                                     \n",
      "10               {'loss': 1.0053302775073263, 'accuracy': 0.730...  \n",
      "20               {'loss': 0.9925603749734921, 'accuracy': 0.676...  \n",
      "30               {'loss': 1.035923654465641, 'accuracy': 0.7165...  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define the text sequence\n",
    "f = open(\"text.txt\", \"r\")\n",
    "text= f.read()\n",
    "# Unique characters in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# Create a mapping from characters to integers and vice versa\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# Length of sequences for training\n",
    "sequence_lengths = [10,20,30]\n",
    "\n",
    "# Function to generate input-output pairs\n",
    "def generate_sequences(text, sequence_length):\n",
    "    sequences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(text) - sequence_length):\n",
    "        sequences.append(text[i:i + sequence_length])\n",
    "        next_chars.append(text[i + sequence_length])\n",
    "    return sequences, next_chars\n",
    "\n",
    "# Generate sequences for each length\n",
    "sequences_data = {}\n",
    "for length in sequence_lengths:\n",
    "    sequences, next_chars = generate_sequences(text, length)\n",
    "    sequences_data[length] = {'sequences': sequences, 'next_chars': next_chars}\n",
    "\n",
    "# Convert sequences to numerical representation\n",
    "for length, data in sequences_data.items():\n",
    "    sequences = data['sequences']\n",
    "    next_chars = data['next_chars']\n",
    "    X = np.zeros((len(sequences), length, vocab_size), dtype=np.float32)\n",
    "    y = np.zeros((len(sequences), vocab_size), dtype=np.float32)\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for t, char in enumerate(sequence):\n",
    "            X[i, t, char_to_int[char]] = 1\n",
    "        y[i, char_to_int[next_chars[i]]] = 1\n",
    "    sequences_data[length]['X'] = torch.from_numpy(X)\n",
    "    sequences_data[length]['y'] = torch.from_numpy(y)\n",
    "\n",
    "# Define RNN, LSTM, and GRU models\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])  \n",
    "        return out\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  \n",
    "        return out\n",
    "\n",
    "class CharGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(CharGRU, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        out = self.fc(out[:, -1, :])  \n",
    "        return out\n",
    "\n",
    "# Define training parameters\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "hidden_size = 128\n",
    "\n",
    "# Train and evaluate models for each sequence length\n",
    "results = {}\n",
    "for length, data in sequences_data.items():\n",
    "    X = data['X']\n",
    "    y = data['y']\n",
    "    input_size = X.shape[-1]\n",
    "\n",
    "    for model_type, Model in [('RNN', CharRNN), ('LSTM', CharLSTM), ('GRU', CharGRU)]:\n",
    "        model = Model(input_size, hidden_size)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "        print(f\"Training {model_type} model with sequence length {length}...\")\n",
    "        start_time = time.time()\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for i in range(0, len(X), batch_size):\n",
    "                inputs = X[i:i+batch_size]\n",
    "                labels = torch.argmax(y[i:i+batch_size], dim=1)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(X)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "\n",
    "    # Evaluate model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total = y.size(0)\n",
    "            correct = (predicted == torch.argmax(y, dim=1)).sum().item()\n",
    "            accuracy = correct / total\n",
    "        # Model complexity\n",
    "        model_size = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "        results.setdefault(model_type, {}).setdefault(length, {})\n",
    "        results[model_type][length]['loss'] = epoch_loss\n",
    "        results[model_type][length]['accuracy'] = accuracy\n",
    "        results[model_type][length]['execution_time'] = execution_time,\n",
    "        \n",
    "\n",
    "        print(f\"Model {model_type} with sequence length {length} - Training Loss: {epoch_loss:.4f}, Training Accuracy: {accuracy:.4f}, Execution Time: {execution_time:.2f} seconds, Model Size: {model_size} parameters\\n\")\n",
    "\n",
    "        # Predict next character for a sequence\n",
    "        test_sequence = sequences_data[length]['sequences'][0]  \n",
    "        X_test = torch.unsqueeze(sequences_data[length]['X'][0], 0)  \n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_test)\n",
    "            _, predicted_index = torch.max(outputs, 1)\n",
    "            predicted_char = int_to_char[int(predicted_index)]\n",
    "            print(f\"Next predicted character for sequence '{test_sequence}': {predicted_char}\\n\")\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.index.name = 'Sequence Length'\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd92c67e",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "2) Build the model for.LSTM and rnn.GRU for the tiny Shakespeare dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d010ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM for sequence length 20\n",
      "Epoch 1/1, Train Loss: 1.676694, Test Loss: 1.490935, Accuracy: 0.5449\n",
      "\n",
      "Training LSTM for sequence length 30\n",
      "Epoch 1/1, Train Loss: 1.663291, Test Loss: 1.469452, Accuracy: 0.5517\n",
      "\n",
      "Training GRU for sequence length 20\n",
      "Epoch 1/1, Train Loss: 1.640184, Test Loss: 1.496257, Accuracy: 0.5427\n",
      "\n",
      "Training GRU for sequence length 30\n",
      "Epoch 1/1, Train Loss: 1.632545, Test Loss: 1.484360, Accuracy: 0.5477\n",
      "\n",
      "LSTM Model Results:\n",
      "\n",
      "Results for sequence length 20:\n",
      "Train Loss: [1.676693968265411]\n",
      "Test Loss: [1.490935357356988]\n",
      "Accuracy: [0.5449198699988793]\n",
      "Execution Time: 1706.71955037117\n",
      "\n",
      "Results for sequence length 30:\n",
      "Train Loss: [1.6632906275785866]\n",
      "Test Loss: [1.4694518446580336]\n",
      "Accuracy: [0.5517476341825321]\n",
      "Execution Time: 4389.573019742966\n",
      "\n",
      "\n",
      "GRU Model Results:\n",
      "\n",
      "Results for sequence length 20:\n",
      "Train Loss: [1.6401844349012795]\n",
      "Test Loss: [1.4962571951690515]\n",
      "Accuracy: [0.54270088535246]\n",
      "Execution Time: 17468.664876699448\n",
      "\n",
      "Results for sequence length 30:\n",
      "Train Loss: [1.6325452436302423]\n",
      "Test Loss: [1.4843602126450355]\n",
      "Accuracy: [0.54765032074702]\n",
      "Execution Time: 4414.790065526962\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Step 1: Download the dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text \n",
    "\n",
    "# Step 2: Prepare the dataset\n",
    "sequence_lengths = [20, 30]\n",
    "\n",
    "# Create a character mapping to integers\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "\n",
    "# Encode the text into integers\n",
    "encoded_text = [char_to_int[ch] for ch in text]\n",
    "\n",
    "# Define dataset class\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequence, target):\n",
    "        self.sequence = sequence\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequence[idx], self.target[idx]\n",
    "\n",
    "# Define LSTM and GRU models\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, rnn_type='lstm'):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        if rnn_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        elif rnn_type == 'gru':\n",
    "            self.rnn = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        out, _ = self.rnn(embed)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Define training function\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    accuracies = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "\n",
    "        test_loss = test_loss / len(test_loader)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    return train_losses, test_losses, accuracies, execution_time\n",
    "\n",
    "# Step 3: Create data loaders\n",
    "batch_size = 128\n",
    "train_loaders = []\n",
    "test_loaders = []\n",
    "for seq_length in sequence_lengths:\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(0, len(encoded_text) - seq_length):\n",
    "        seq = encoded_text[i:i+seq_length]\n",
    "        target = encoded_text[i+seq_length]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "\n",
    "    sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "    targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "    dataset = CharDataset(sequences, targets)\n",
    "    train_size = int(len(dataset) * 0.8)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    train_loaders.append(train_loader)\n",
    "    test_loaders.append(test_loader)\n",
    "\n",
    "# Step 4: Define hyperparameters\n",
    "input_size = len(chars)\n",
    "hidden_size = 256\n",
    "output_size = len(chars)\n",
    "num_layers = 2\n",
    "epochs = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Step 5: Initialize and train LSTM models\n",
    "lstm_train_losses = []\n",
    "lstm_test_losses = []\n",
    "lstm_accuracies = []\n",
    "lstm_execution_times = []\n",
    "for i, seq_length in enumerate(sequence_lengths):\n",
    "    print(f\"\\nTraining LSTM for sequence length {seq_length}\")\n",
    "    lstm_model = CharRNN(input_size, hidden_size, output_size, num_layers, rnn_type='lstm')\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate)\n",
    "    lstm_train_loss, lstm_test_loss, lstm_accuracy, lstm_execution_time = train_model(lstm_model, train_loaders[i], test_loaders[i], criterion, optimizer, epochs)\n",
    "    lstm_train_losses.append(lstm_train_loss)\n",
    "    lstm_test_losses.append(lstm_test_loss)\n",
    "    lstm_accuracies.append(lstm_accuracy)\n",
    "    lstm_execution_times.append(lstm_execution_time)\n",
    "\n",
    "# Step 6: Initialize and train GRU models\n",
    "gru_train_losses = []\n",
    "gru_test_losses = []\n",
    "gru_accuracies = []\n",
    "gru_execution_times = []\n",
    "for i, seq_length in enumerate(sequence_lengths):\n",
    "    print(f\"\\nTraining GRU for sequence length {seq_length}\")\n",
    "    gru_model = CharRNN(input_size, hidden_size, output_size, num_layers, rnn_type='gru')\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(gru_model.parameters(), lr=learning_rate)\n",
    "    gru_train_loss, gru_test_loss, gru_accuracy, gru_execution_time = train_model(gru_model, train_loaders[i], test_loaders[i], criterion, optimizer, epochs)\n",
    "    gru_train_losses.append(gru_train_loss)\n",
    "    gru_test_losses.append(gru_test_loss)\n",
    "    gru_accuracies.append(gru_accuracy)\n",
    "    gru_execution_times.append(gru_execution_time)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nLSTM Model Results:\")\n",
    "for i, seq_length in enumerate(sequence_lengths):\n",
    "    print(f\"\\nResults for sequence length {seq_length}:\")\n",
    "    print(\"Train Loss:\", lstm_train_losses[i])\n",
    "    print(\"Test Loss:\", lstm_test_losses[i])\n",
    "    print(\"Accuracy:\", lstm_accuracies[i])\n",
    "    print(\"Execution Time:\", lstm_execution_times[i])\n",
    "\n",
    "print(\"\\n\\nGRU Model Results:\")\n",
    "for i, seq_length in enumerate(sequence_lengths):\n",
    "    print(f\"\\nResults for sequence length {seq_length}:\")\n",
    "    print(\"Train Loss:\", gru_train_losses[i])\n",
    "    print(\"Test Loss:\", gru_test_losses[i])\n",
    "    print(\"Accuracy:\", gru_accuracies[i])\n",
    "    print(\"Execution Time:\", gru_execution_times[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d91d245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
